{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Regression, the LASSO and Ridge"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Last revision: Thu 2 June 2022 2:32:00 AEST"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Author: Omar Ghattas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\hat{w} = (X^T X)^{-1} X^T y$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7fb085f52010>]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "X = [3, 6, 7, 8, 11]\n",
    "y = [13, 8, 11, 2, 6]\n",
    "\n",
    "plt.scatter(X, y)\n",
    "\n",
    "X = np.stack((X, np.ones(len(X))), axis=1)\n",
    "\n",
    "w_hat = np.linalg.inv(X.T @ X) @ X.T @ y\n",
    "plt.plot(X[:, 0], X @ w_hat, c='red')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this lab we will explore linear models in more depth, specifically:\n",
    "\n",
    "1. Linear regression and how to run a linear regression model in the Python library `scikit-klearn`, also commonly referred to as `sklearn`. The focus here will be on practical aspects of the modelling, and you should look at the tutorial this week to understand what is going on under the hood. It is important for you to understand both the theory and pratical side of the concepts we cover throughout the course.\n",
    "\n",
    "2. Regularized linear models, namely the LASSO and Ridge models. \n",
    "\n",
    "**Note: this notebook has only been tested using Python 3.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### What is `sklearn` ?\n",
    "Throughout this course, we will be relying heavily on the `sklearn` module which contains implementations of many of the algorithms that will be covered. The following links to a summary of the different types of algorithms available, you will likely need to make use of all of them at some point in the course:\n",
    "\n",
    "https://scikit-learn.org/stable/\n",
    "\n",
    "Here is a short summary of what the different sections mean:\n",
    "\n",
    "1. Classification: contains implementations of classification models such as SVM and Naive Bayes. Classification refers to the case where we have access to data of the form $(x_i, c_i)$, where $x_i$ is a collection of features, and $c_i$ is a class label. For example, in an image classification task, we might have access to images of cats and dogs, and wish to build a classifier that takes as input $x_i$ (a collection of pixels), and predicts the correct class label (0 for cat, 1 for dog). Importantly, the class label is always one of a discrete number of possible choices.\n",
    "\n",
    "2. Regression: Similar to classification, except that now we have data $(x_i, y_i)$ and $y_i$ is a continuous value. For example, $x_i$ could represent information about a patient (e.g. age, weight, height, blood pressure) and $y_i$ could represent the patients chance of recovering from a particular surgery.\n",
    "\n",
    "3. Clustering: Both 1. and 2. are examples of supervised learning, since we have access to a label ($c_i$) or target value $(y_i)$. In many cases of interest, we do not have access to a label, we only have access to $x_i$. For example, you have access to 100,000 machine-readable texts. You would like to understand which of the texts are 'similar'. In this case you might employ a clustering algorithm to group similiar texts into the same clusters. If your clustering algorithm is successful, your clusters might correspond to texts written by the same author, or texts of the same genre, etc.\n",
    "\n",
    "4. Dimensionality reduction: Often the $x_i$ in categories of 1., 2., 3. can be massive. For example, an 3.1 megapixel image is 2048 pixels in width, and 1536 pixels in height, and can be represented as a matrix of pixel values:\n",
    "\n",
    "$$\n",
    "x_i = \\begin{bmatrix}\n",
    "p_{1,1}&p_{1,2} &p_{1,3} & \\cdots p_{1,2048}\\\\\n",
    "p_{2,1}&p_{2,2} &p_{2,3} & \\cdots p_{2,2048}\\\\\n",
    "\\vdots&\\vdots &\\vdots & \\cdots \\vdots\\\\\n",
    "p_{1536,1}&p_{1536,2} &p_{1536,3} & \\cdots p_{1536,2048}\\\\\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "where each element represents the brightness of that particular portion of the image. Note that we therefore have a total of ~3.1 million pixels. In many cases, we don't really believe that we need this many pixels to understand what is contained in the image, and more importantly, it is often difficult to run a machine learning algorithm on such a high dimensional problem in a reasonable amount of time. For example, an image of a dog can be identified by noting that the image contains pointy ears, a tail, and a black nose, and this information might be contained in a collection of 10,000 pixels. It is therefore important to understand how one might reduce an input $x_i \\in \\mathbb{R}^p$ to some lower dimensional $z_i \\in \\mathbb{R}^k$, where $k \\ll p$, and where $z_i$ retains much of the information contained in $x_i$. This is what the algorithms in this sub-module do.\n",
    "\n",
    "5. Model Selection: it is important to not only know how to build a model, but also be able to pick the best model out of a collection of models. This sub-module contains algorithms to do just that. It also contains important functions that allow us to perform cross validation to choose hyperparameters, something that will be important to us in future weeks (and for the final project).\n",
    "\n",
    "6. Preprocessing: Before running a machine learning algorithm, we usually need to perform a fair amount of preprocessing to the original data before. For example, we might wish to scale our feature vectors to be in the range [0,1], or we might want to handle missing data, or generate new features.\n",
    "\n",
    "You are encouraged to refer to this link regularly. In this lab, we will focus on linear regression, which is contained in the Regression section:\n",
    "\n",
    "https://scikit-learn.org/stable/supervised_learning.html#supervised-learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Linear Models in `sklearn`\n",
    "\n",
    "In the following, we will:\n",
    "\n",
    "1. use `sklearn` to create a toy dataset with a specified number of samples ($n$) and features ($p$)\n",
    "2. split the data into train and test sets\n",
    "3. fit a linear regression model, a LASSO model, and a ridge model to the training data. For reference, recall that the three models solve the following different objectives:\n",
    "- linear model: $\\hat{\\beta} = \\arg \\min_{\\beta} \\frac{1}{n}\\|y-X \\beta\\|^2_2$\n",
    "- LASSO model: $\\hat{\\beta}_{\\text{LASSO}} = \\arg \\min_{\\beta} \\frac{1}{n}\\|y-X \\beta\\|^2_2 + \\lambda \\| \\beta \\|_1$\n",
    "- Ridge model: $\\hat{\\beta}_{\\text{Ridge}} = \\arg \\min_{\\beta} \\frac{1}{n}\\|y-X \\beta\\|^2_2 + \\alpha \\| \\beta \\|_2^2$\n",
    "4. Evaluate each of the models on the test data and compare them using various metrics\n",
    "\n",
    "The MSE term is a `data-fit' term, which measures how well the model fits the data by looking at the distance of the true $y$ values to the predicted values $\\hat{y} = X\\beta$, and the second term (in LASSO and ridge) is a penalty term that increases as the length of the solution increases. Length is a proxy for the complexity of the model, and so we can think of this term as preferring simpler models to more complex ones. LASSO and Ridge therefore represent a trade-off: on the one hand they would like to get a perfect fit and reduce the first term to zero, but this can only really be done with very complex models that will lead the penalty term to become very large. Note also that $\\alpha$ and $\\lambda$ are known as hyper-parameters, whereas we generally refer to $\\beta$ as the unknown parameter, or weights of the model.\n",
    "\n",
    "Throughout, pay close attention to which sub-modules are being used, and check out the `sklearn` documentation which is often an excellent reference. Also note that we are fitting the most basic form of each model, and you should dig deeper to understand the different arguments available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression, Lasso, Ridge\n",
    "from sklearn.datasets import make_regression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error \n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set a random seed for reproducibility\n",
    "rs = np.random.RandomState(123)\n",
    "\n",
    "# create a dataset of X, y pairs\n",
    "n = 1000                   # number of samples\n",
    "p = 10                     # number of features\n",
    "noise = 0.4                # add noise to the data\n",
    "nmb_informative=p//2       # how many of the p feature are actually useful for prediction\n",
    "X, y = make_regression(n, p, \n",
    "                       noise=noise, \n",
    "                       n_informative=nmb_informative, \n",
    "                       random_state=rs)\n",
    "\n",
    "# split into training data and test data\n",
    "train_prop = 0.7           # proportion of data in train\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=rs)\n",
    "\n",
    "# fit models to train data\n",
    "m_linearReg = LinearRegression().fit(X_train, y_train)\n",
    "m_lasso = Lasso().fit(X_train, y_train)\n",
    "m_ridge = Ridge().fit(X_train, y_train)\n",
    "\n",
    "# compute predictions of each model on the train data\n",
    "ypred_train_linearReg = m_linearReg.predict(X_train)\n",
    "ypred_train_lasso = m_lasso.predict(X_train)\n",
    "ypred_train_ridge = m_ridge.predict(X_train)\n",
    "\n",
    "# compute predictions of each model on the test data\n",
    "ypred_test_linearReg = m_linearReg.predict(X_test)\n",
    "ypred_test_lasso = m_lasso.predict(X_test)\n",
    "ypred_test_ridge = m_ridge.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the `sklearn` implementations all have the same methods, so understanding how the `linearRegression` implementation works means you pretty much understand how any other regression algorithm in `sklearn` works too. \n",
    "\n",
    "Now, we have fit various models, and wish to evaluate them. We can use a pandas dataframe to make this look nice:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pandas'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# first compute metrics and store them in array\u001b[39;00m\n\u001b[1;32m      4\u001b[0m loss_data \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m      5\u001b[0m     [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlinear regression\u001b[39m\u001b[38;5;124m'\u001b[39m, \n\u001b[1;32m      6\u001b[0m      mean_squared_error(y_train, ypred_train_linearReg), \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     22\u001b[0m     ]\n\u001b[1;32m     23\u001b[0m ]\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'pandas'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# first compute metrics and store them in array\n",
    "loss_data = [\n",
    "    ['linear regression', \n",
    "     mean_squared_error(y_train, ypred_train_linearReg), \n",
    "     mean_squared_error(y_test, ypred_test_linearReg), \n",
    "     mean_absolute_error(y_train, ypred_train_linearReg), \n",
    "     mean_absolute_error(y_test, ypred_test_linearReg)\n",
    "    ],\n",
    "    ['lasso', \n",
    "     mean_squared_error(y_train, ypred_train_lasso), \n",
    "     mean_squared_error(y_test, ypred_test_lasso),\n",
    "     mean_absolute_error(y_train, ypred_train_lasso), \n",
    "     mean_absolute_error(y_test, ypred_test_lasso)\n",
    "    ],\n",
    "    ['ridge', \n",
    "     mean_squared_error(y_train, ypred_train_ridge), \n",
    "     mean_squared_error(y_test, ypred_test_ridge),\n",
    "     mean_absolute_error(y_train, ypred_train_ridge), \n",
    "     mean_absolute_error(y_test, ypred_test_ridge)\n",
    "    ]\n",
    "]\n",
    "\n",
    "# create a pandas dataframe and feed it the array and naming the columns\n",
    "loss_df = pd.DataFrame(loss_data, \n",
    "                       columns=['Model', 'train MSE', 'test MSE', 'train MAE', 'test MAE'])\n",
    "print(loss_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " <font color='purple'>\n",
    "\n",
    "#### Exercise:\n",
    "    \n",
    "Can you think of a good reason why the lasso performs so poorly in this scenario? Hint: hyperparameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One way to (algorithmically) choose a better $\\lambda$ is to perform cross validation, which we will explore in more depth a little later in the lab. For now, we note that sklearn has an in built object that does LASSO regression that automatically performs cross validation for the choice of $\\lambda$, called `LassoCV` - note that there is also a `RidgeCV` object available. Choosing $\\lambda$ properly should make a big difference in our results. We use it as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LassoCV\n",
    "\n",
    "m_lassoCV = LassoCV().fit(X_train, y_train)                 # automatically perform CV for choice of lambda\n",
    "ypred_train_lassoCV = m_lassoCV.predict(X_train)\n",
    "ypred_test_lassoCV = m_lassoCV.predict(X_test)\n",
    "\n",
    "# add row to loss_df\n",
    "lassoCV_row = {'Model': 'lasso CV', \n",
    "               'train MSE': mean_squared_error(y_train, ypred_train_lassoCV), \n",
    "               'test MSE': mean_squared_error(y_test, ypred_test_lassoCV),\n",
    "               'train MAE': mean_absolute_error(y_train, ypred_train_lassoCV), \n",
    "               'test MAE' :mean_absolute_error(y_test, ypred_test_lassoCV)}\n",
    "\n",
    "loss_df.append(lassoCV_row, ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a minute to inspect our fitted linear model object. Recall that a linear model just minimizes an objective:\n",
    "$$\n",
    "\\hat{\\beta} = \\arg \\min_{\\beta} = \\frac{1}{n} \\| y-X\\beta\\|^2_2\n",
    "$$\n",
    "If we want to access the estimate computed, i.e. the value of $\\hat{\\beta}$, we can do so by running\n",
    "\n",
    "\n",
    "- $\\hat{\\beta}_0 \\leftrightarrow$ `m_linearReg.intercept_`\n",
    "- $[\\hat{\\beta}_1, \\hat{\\beta}_2, \\dots, \\hat{\\beta}_p ] \\leftrightarrow$ `m_linearReg.coef_`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What's in a feature?\n",
    "In this section, let's investigate how choosing the wrong features can impact the performance of our model. We will work with a single feature in this case. We will specify a \"true\" function $f(x)=x^2$, and then create a `data generating process' which will let us sample noisy observations from $f$. We do this by sampling:\n",
    "\n",
    "$$\n",
    "y = f(x) + \\epsilon, \\qquad \\epsilon \\sim N(0, \\sigma^2).\n",
    "$$\n",
    "\n",
    "In other words, each observation $y$ in our dataset is $f$ evaluated at the point $x$, plus some random Gaussian noise. We will first generate $n$ data points in this way, and then try to fit a model to the resulting dataset. We will fit two models:\n",
    "\n",
    "1. bad model: $\\hat{y} = b_0 + b_1 x$\n",
    "1. good model: $\\tilde{y} = w_0 + w_1 x^2$\n",
    "\n",
    "We call the first model a bad model since we know that the true underlying function is the square function, but we are only including $x$ in our model. The second model is the good model because it uses the correct feature. We will work with a single feature so that we can easily visualise the different model fits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 40\n",
    "X = np.random.uniform(-1, 1, size=(n,1))          # generate 100 points uniformly on [-1,1] and shape into column\n",
    "noise = np.random.normal(0, 0.2, size=(n,1))      # generate 100 noise variables with sigma=0.2\n",
    "f = lambda x: x**2                                # true function f(x) = x^2\n",
    "y = f(X) + noise                                  # noisy observations\n",
    "\n",
    "bad_mod = LinearRegression().fit(X, y)\n",
    "good_mod = LinearRegression().fit(np.concatenate((X,X**2), axis=1), y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualization code - see intro lab for more details on plotting\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14,5))\n",
    "xrange = np.linspace(-1,1,1000)\n",
    "\n",
    "# first axis\n",
    "ax1.plot(xrange, f(xrange), color='blue', label='truth')\n",
    "ax1.scatter(X, y, color='orange', label='noisy samples')\n",
    "\n",
    "# create function for bad_mod\n",
    "bad_lr = lambda x: bad_mod.intercept_[0] + bad_mod.coef_[0][0]*x\n",
    "\n",
    "ax1.plot(xrange, bad_lr(xrange), color='green', label='bad_mod')\n",
    "ax1.legend()\n",
    "ax1.set_title(\"bad model, $\\hat{y} = b_0 + b_1 x$\")\n",
    "\n",
    "# second axis\n",
    "ax2.plot(xrange, f(xrange), color='blue', label='truth')\n",
    "ax2.scatter(X, y, color='orange', label='noisy samples')\n",
    "# create function for good_mod\n",
    "good_lr = lambda x: good_mod.intercept_[0] + good_mod.coef_[0][0]*x + good_mod.coef_[0][1]*x**2\n",
    "ax2.plot(xrange, good_lr(xrange), color='green', label='good_mod')\n",
    "ax2.legend()\n",
    "ax2.set_title(\"good model, $\\\\tilde{y} = w_0 + w_1 x^2$\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the above exercise, we can see that it is absolutely crucial to think about what features to include in your model.\n",
    "\n",
    " <font color='purple'>\n",
    "\n",
    "#### Exercise:\n",
    "    \n",
    "Repeat the above experiment with a different true function, plot the results and compute the MSE between the bad and good models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploring a real dataset\n",
    "We now look at a real-life dataset contained in `life_expectancy.csv` and introduce some techniques for data exploration. Data exploration is important for identifying both model and possible features to include in your model. In the previous example, we knew exactly what the 'true' model was, but in reality we only have access to the data and must somehow use this data to learn a good estimate of the truth. The following is a rough guide for what should be done with real data and you should expand on these techniques throughout the course (especially in your group projects).\n",
    "\n",
    "Data summary: the 9 features in the dataset represent Infant deaths, Alcohol, Percentage expenditure, Hepatitis B, Measles, BMI, Total expenditure, Population and Schooling. The target of interest is Life expectancy. There are a total of 500 records.\n",
    "\n",
    "Let's read in the data and generate a few exploratory plots. Usually we are looking to understand:\n",
    "\n",
    "1. The distribution of the target variable and the explanatory variables (the features)\n",
    "2. Are there any noticeable outliers?\n",
    "3. What is the relationship between the target variable and the features? If it is linear, then a linear model is a good choice for example.\n",
    "\n",
    "This process is usually called the 'Exploratory Data Analysis (EDA)' stage. We will plot using the `seabord` module which produces some nice plots easily when working with pandas dataframes, but all of these plots can be created using matplotlib of course."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "le_df = pd.read_csv(\"life_expectancy.csv\")         # loading in data using pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.boxplot(x=le_df['Life expectancy'])            # inspecting distribution of target variable using a boxplot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we look at the correlations between the target variable (Life expectancy) and the other variables. If features are highly correlated (either positively or ngetaively) then they should be good additions to the model. Note however that these correlations only capture linear correlations, and it is possible for two variables to be related even if their correlation is zero. See here for some nice examples:\n",
    "\n",
    "https://blog.revolutionanalytics.com/2017/05/the-datasaurus-dozen.html\n",
    "\n",
    "Note that a correlation matrix is symmetric, so elements above the diagonal are identical to those below the diagonal. For this reason we only need to view the lower triangular portion of the matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_mat = le_df.corr()                               # compute the correlation matrix\n",
    "\n",
    "mask = np.zeros_like(corr_mat, dtype=bool)            # mask out upper triangle (correlation matrices are symmetric)\n",
    "mask[np.triu_indices_from(mask)] = True\n",
    "\n",
    "\n",
    "f, ax = plt.subplots(figsize=(11, 9))\n",
    "cmap = sns.diverging_palette(220, 10, as_cmap=True)   # generate custom colormap\n",
    "sns.heatmap(corr_mat, mask=mask, cmap=cmap, vmax=.3, center=0,\n",
    "            square=True, linewidths=.5, cbar_kws={\"shrink\": .5})\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another important type of plot is the pair plot, which plots a scatter of each variable against every other variable. Note that the diagonals contain histograms of each variable. We usually want to look for variables that seem to be related based on their scatters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.pairplot(le_df)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " <font color='purple'>\n",
    "\n",
    "#### Extended Exercise: LASSO vs. Ridge\n",
    "In this exercise, we will explore the differences between LASSO and Ridge regression models, and we will take a deep look at cross validation and how it works for choosing hyper-parameters for these models. You need to do the following:\n",
    "\n",
    "Q1. Consider the dataset: `data.csv`, with response $Y$ and features $X_1,\\dots, X_8$. Create a pairs plot to study the correlations between the features. Describe what you see and how this might affect a linear regression model.\n",
    "\n",
    "Q2. In order for LASSO and Ridge to be run properly, we often rescale the features in the dataset. Explain Why. Then, use the `sklearn.preprocessing.StandardScaler` function to scale your features. Print out the mean and variance of each of the 8 features (after they have been scaled).\n",
    "\n",
    "Q3. Now we will apply ridge regression to this dataset, recall that ridge regression is defined as the solution to the optimisation:\n",
    "    \\begin{align*}\n",
    "        \\hat{\\beta} = \\arg\\min \\left \\{ \\frac{1}{2}\\|Y-X\\beta \\|^2_2 + \\lambda \\| \\beta\\|_2^2\\right \\}.\n",
    "    \\end{align*}\n",
    "    Run ridge regression with $\\lambda = \\{0.01,0.1,0.5,1,1.5, 2,5,10, 20, 30,50, 100, 200, 300\\}$. Create a plot with $x$-axis representing $\\log(\\lambda)$, and $y$-axis representing the value of the coefficient for each feature in each of the fitted ridge models. In other words, the plot should describe what happens to each of the coefficients in your model for the different choices of $\\lambda$. For this problem you are permitted to use the \\texttt{sklearn} implementation of Ridge regressiom to run the models and extract the coefficients, and base matplotlib/numpy to create the plots but no other packages are to be used to generate this plot. In a few lines, comment on what you see, in particular what do you observe for features $3, 4, 5$?\n",
    "    \n",
    "Q4. Now, use Leave-One-Out Cross Validation (LOOCV) to find a good value of $\\lambda$ for the ridge problem. Create a  grid of $\\lambda$ values running from $0$ to $50$ in increments of $0.1$, so the grid would be: $0, 0.1, 0.2,\\dots, 50$. For each data point $i=1,\\dots, n$, run ridge with each $\\lambda$ value on the dataset with point $i$ removed, find $\\hat{\\beta}$, then get the leave-one-out error for predicting $Y_i$. Average the squared error over all $n$ choices of $i$. Plot the leave-one-out error against $\\lambda$ and find the best $\\lambda$ value. Compare your results to standard Ordinary Least Squares (OLS), does the ridge seem to give better prediction error based on your analysis? Note that for this question you are \\textbf{not} permitted to use any existing packages that implement cross validation, you must write the code yourself from scratch. You must create the plot yourself from scratch using basic matplotlib functionality.\n",
    "\n",
    "Q5. Repeat Q3 but for the LASSO\n",
    "\n",
    "Q6. Repeat Q4 but for the LASSO\n",
    "\n",
    "Q7. Comment on what you observe and which model you would prefer in a prediction problem. What about an inference problem?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
